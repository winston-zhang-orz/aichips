{
  "chips": [
    {
      "id": "nvidia-h100",
      "name": "H100",
      "company": "NVIDIA",
      "country": "USA",
      "launch_date": "2022-03",
      "process_node": "4nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC", "LLM Training"],
      "memory": "80GB HBM3",
      "peak_tflops_bf16": 989,
      "interconnect": "NVLink 4.0 / PCIe 5.0",
      "tdp_watts": 700,
      "price_usd_approx": 30000,
      "status": "Mass Production",
      "notes": "Flagship data center GPU, backbone of most LLM training clusters"
    },
    {
      "id": "nvidia-h200",
      "name": "H200",
      "company": "NVIDIA",
      "country": "USA",
      "launch_date": "2023-11",
      "process_node": "4nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC", "LLM Training"],
      "memory": "141GB HBM3e",
      "peak_tflops_bf16": 989,
      "interconnect": "NVLink 4.0 / PCIe 5.0",
      "tdp_watts": 700,
      "price_usd_approx": 35000,
      "status": "Mass Production",
      "notes": "H100 with larger HBM3e memory for inference of very large models"
    },
    {
      "id": "nvidia-b200",
      "name": "B200",
      "company": "NVIDIA",
      "country": "USA",
      "launch_date": "2024-03",
      "process_node": "4nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC", "Next-Gen LLM"],
      "memory": "192GB HBM3e",
      "peak_tflops_bf16": 4500,
      "interconnect": "NVLink 5.0",
      "tdp_watts": 1000,
      "price_usd_approx": 40000,
      "status": "Announced / Shipping",
      "notes": "Blackwell architecture, 4.5x performance over H100 in FP8"
    },
    {
      "id": "nvidia-a100",
      "name": "A100",
      "company": "NVIDIA",
      "country": "USA",
      "launch_date": "2020-05",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC"],
      "memory": "80GB HBM2e",
      "peak_tflops_bf16": 312,
      "interconnect": "NVLink 3.0 / PCIe 4.0",
      "tdp_watts": 400,
      "price_usd_approx": 10000,
      "status": "Mass Production",
      "notes": "Ampere architecture, widely deployed, export-restricted to China since 2022"
    },
    {
      "id": "amd-mi300x",
      "name": "MI300X",
      "company": "AMD",
      "country": "USA",
      "launch_date": "2023-12",
      "process_node": "5nm / 6nm (TSMC) chiplet",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC", "LLM Inference"],
      "memory": "192GB HBM3",
      "peak_tflops_bf16": 1307,
      "interconnect": "PCIe 5.0 / Infinity Fabric",
      "tdp_watts": 750,
      "price_usd_approx": 15000,
      "status": "Mass Production",
      "notes": "Largest HBM capacity in market, strong inference competitor to H100"
    },
    {
      "id": "google-tpuv5",
      "name": "TPU v5e",
      "company": "Google",
      "country": "USA",
      "launch_date": "2023-08",
      "process_node": "Proprietary",
      "type": "Training / Inference",
      "target_market": ["Cloud AI (Google Cloud)", "LLM Training"],
      "memory": "16GB HBM",
      "peak_tflops_bf16": 393,
      "interconnect": "ICI (Inter-Chip Interconnect)",
      "tdp_watts": 170,
      "price_usd_approx": null,
      "status": "Cloud Only (Google Cloud)",
      "notes": "Cloud-only, available via GCP; optimized for cost-efficiency"
    },
    {
      "id": "aws-trainium2",
      "name": "Trainium2",
      "company": "AWS",
      "country": "USA",
      "launch_date": "2023-11",
      "process_node": "Proprietary (TSMC)",
      "type": "Training",
      "target_market": ["Cloud AI (AWS)", "LLM Training"],
      "memory": "96GB HBM",
      "peak_tflops_bf16": 832,
      "interconnect": "NeuronLink",
      "tdp_watts": 600,
      "price_usd_approx": null,
      "status": "Cloud Only (AWS)",
      "notes": "4x performance over Trainium1, cloud-only on AWS"
    },
    {
      "id": "intel-gaudi3",
      "name": "Gaudi 3",
      "company": "Intel",
      "country": "USA",
      "launch_date": "2024-04",
      "process_node": "5nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "Enterprise AI"],
      "memory": "128GB HBM2e",
      "peak_tflops_bf16": 1835,
      "interconnect": "PCIe 5.0 / RDMA",
      "tdp_watts": 900,
      "price_usd_approx": 8000,
      "status": "Mass Production",
      "notes": "Formerly Habana Gaudi, open software stack (PyTorch native)"
    },
    {
      "id": "huawei-ascend910b",
      "name": "昇腾 910B (Ascend 910B)",
      "company": "华为 (Huawei)",
      "country": "China",
      "launch_date": "2023-08",
      "process_node": "7nm (SMIC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "LLM Training", "Chinese Market"],
      "memory": "64GB HBM2",
      "peak_tflops_bf16": 320,
      "interconnect": "HCCS",
      "tdp_watts": 400,
      "price_usd_approx": 8000,
      "status": "Mass Production",
      "notes": "China's leading AI training chip, positioned as A100 alternative; CANN software ecosystem"
    },
    {
      "id": "huawei-ascend910c",
      "name": "昇腾 910C (Ascend 910C)",
      "company": "华为 (Huawei)",
      "country": "China",
      "launch_date": "2024-Q2",
      "process_node": "6nm (SMIC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "LLM Training", "Chinese Market"],
      "memory": "96GB HBM",
      "peak_tflops_bf16": 780,
      "interconnect": "HCCS 3.0",
      "tdp_watts": 600,
      "price_usd_approx": 12000,
      "status": "Announced / Early Production",
      "notes": "Next-gen Ascend, targets H100 performance; key to China's AI self-sufficiency"
    },
    {
      "id": "cambricon-mlu590",
      "name": "MLU590",
      "company": "寒武纪 (Cambricon)",
      "country": "China",
      "launch_date": "2023-05",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "LLM Training", "Chinese Market"],
      "memory": "64GB HBM2e",
      "peak_tflops_bf16": 256,
      "interconnect": "MLU-Link",
      "tdp_watts": 350,
      "price_usd_approx": 6000,
      "status": "Mass Production",
      "notes": "China's first publicly-listed AI chip company; supports PyTorch, TensorFlow"
    },
    {
      "id": "cambricon-mlu370",
      "name": "MLU370",
      "company": "寒武纪 (Cambricon)",
      "country": "China",
      "launch_date": "2021-09",
      "process_node": "7nm (TSMC)",
      "type": "Inference",
      "target_market": ["Cloud Inference", "Chinese Market"],
      "memory": "24GB LPDDR5",
      "peak_tflops_bf16": 128,
      "interconnect": "PCIe 4.0",
      "tdp_watts": 150,
      "price_usd_approx": 3000,
      "status": "Mass Production",
      "notes": "Inference-optimized, deployed in major Chinese cloud providers"
    },
    {
      "id": "biren-br100",
      "name": "BR100",
      "company": "壁仞科技 (Biren Technology)",
      "country": "China",
      "launch_date": "2022-08",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC", "Chinese Market"],
      "memory": "64GB HBM2e",
      "peak_tflops_bf16": 256,
      "interconnect": "BIRENSAIL",
      "tdp_watts": 550,
      "price_usd_approx": 10000,
      "status": "Limited Production",
      "notes": "Ambitious specs at launch; impacted by US export controls on TSMC access"
    },
    {
      "id": "enflame-t30",
      "name": "云燧T30 (Yunsui T30)",
      "company": "燧原科技 (Enflame Technology)",
      "country": "China",
      "launch_date": "2023-03",
      "process_node": "7nm (TSMC)",
      "type": "Training",
      "target_market": ["Cloud AI", "LLM Training", "Chinese Market"],
      "memory": "32GB HBM2e",
      "peak_tflops_bf16": 256,
      "interconnect": "GRS (Glue-free Rack Scale)",
      "tdp_watts": 300,
      "price_usd_approx": 5000,
      "status": "Mass Production",
      "notes": "Used by Tencent and other Chinese hyperscalers; GRS high-speed interconnect"
    },
    {
      "id": "iluvatar-biv150",
      "name": "天垓100 (Bi-V150)",
      "company": "天数智芯 (Iluvatar CoreX)",
      "country": "China",
      "launch_date": "2023-06",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "Chinese Market"],
      "memory": "32GB HBM2e",
      "peak_tflops_bf16": 256,
      "interconnect": "PCIe 5.0",
      "tdp_watts": 300,
      "price_usd_approx": 5000,
      "status": "Mass Production",
      "notes": "GPU architecture compatible with CUDA ecosystem migration tools"
    },
    {
      "id": "moorethreads-s4000",
      "name": "MTT S4000",
      "company": "摩尔线程 (Moore Threads)",
      "country": "China",
      "launch_date": "2023-11",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference / Rendering",
      "target_market": ["Cloud AI", "Graphics", "Chinese Market"],
      "memory": "48GB GDDR6X",
      "peak_tflops_bf16": 200,
      "interconnect": "PCIe 4.0 / MUSA Interconnect",
      "tdp_watts": 300,
      "price_usd_approx": 4000,
      "status": "Mass Production",
      "notes": "MUSA GPU architecture supporting graphics + AI; China's domestic GPU play"
    },
    {
      "id": "metax-c500",
      "name": "曦云C500 (MetaX C500)",
      "company": "沐曦 (MetaX)",
      "country": "China",
      "launch_date": "2023-09",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "LLM Training", "Chinese Market"],
      "memory": "64GB HBM2e",
      "peak_tflops_bf16": 256,
      "interconnect": "MXLINK",
      "tdp_watts": 400,
      "price_usd_approx": 7000,
      "status": "Mass Production",
      "notes": "Full-stack GPU with CUDA compatibility layer (MXMACA software stack)"
    },
    {
      "id": "horizon-journey6",
      "name": "征程6 (Journey 6)",
      "company": "地平线 (Horizon Robotics)",
      "country": "China",
      "launch_date": "2023-04",
      "process_node": "7nm",
      "type": "Edge AI / Automotive",
      "target_market": ["Autonomous Driving", "ADAS", "Smart Vehicle"],
      "memory": "16GB LPDDR5",
      "peak_tflops_bf16": 560,
      "interconnect": "PCIe / Ethernet",
      "tdp_watts": 70,
      "price_usd_approx": 200,
      "status": "Mass Production",
      "notes": "L2-L4 autonomous driving chip; deployed in Li Auto, BAIC and more"
    },
    {
      "id": "blacksesame-a1000",
      "name": "华山A1000 Pro",
      "company": "黑芝麻智能 (Black Sesame Technologies)",
      "country": "China",
      "launch_date": "2022-12",
      "process_node": "16nm",
      "type": "Edge AI / Automotive",
      "target_market": ["Autonomous Driving", "ADAS"],
      "memory": "4GB LPDDR4X",
      "peak_tflops_bf16": 196,
      "interconnect": "PCIe / Ethernet",
      "tdp_watts": 36,
      "price_usd_approx": 100,
      "status": "Mass Production",
      "notes": "L3+ autonomous driving, listed on HK Stock Exchange in 2024"
    },
    {
      "id": "hygon-dcu",
      "name": "深算DCU K100",
      "company": "海光信息 (Hygon)",
      "country": "China",
      "launch_date": "2023-06",
      "process_node": "7nm (TSMC)",
      "type": "Training / Inference",
      "target_market": ["Cloud AI", "HPC", "Chinese Market"],
      "memory": "32GB HBM2e",
      "peak_tflops_bf16": 200,
      "interconnect": "PCIe 4.0 / HCCL",
      "tdp_watts": 250,
      "price_usd_approx": 4000,
      "status": "Mass Production",
      "notes": "AMD GPU architecture derivative; ROCm compatible; strong HPC legacy"
    },
    {
      "id": "rockchip-rk3588",
      "name": "RK3588",
      "company": "瑞芯微 (Rockchip)",
      "country": "China",
      "launch_date": "2022-02",
      "process_node": "8nm (TSMC)",
      "type": "Edge AI / Embedded",
      "target_market": ["Edge AI", "IoT", "Embedded Systems"],
      "memory": "Up to 32GB LPDDR5",
      "peak_tflops_bf16": 6,
      "interconnect": "PCIe 3.0 / USB 3.0",
      "tdp_watts": 10,
      "price_usd_approx": 20,
      "status": "Mass Production",
      "notes": "6 TOPS NPU, widely used in edge AI applications, NAS, thin clients"
    }
  ]
}
